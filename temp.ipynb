{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from common.lora_resize import change_lora_rank\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "\n",
    "path = \"common/lora_outputs\"\n",
    "files = sorted(glob.glob(f\"{path}/**/lora_layers.pth\", recursive=True))\n",
    "new_files = sorted(glob.glob(f\"{path}/**/lora_layers_new.pth\", recursive=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = 'common/lora_outputs/lora-Anne Hathaway/lora_layers.pth'\n",
    "path2 = 'common/lora_outputs/lora-Anne Hathaway/lora_layers_new.pth'\n",
    "\n",
    "state_dict1 = torch.load(path1)\n",
    "state_dict2 = torch.load(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "down1 = state_dict1['unet']['down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_down.weight']\n",
    "up1 = state_dict1['unet']['down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_up.weight']\n",
    "\n",
    "down2 = state_dict2['down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_down.weight']\n",
    "up2 = state_dict2['down_blocks.0.attentions.0.transformer_blocks.0.attn2.to_out.0.lora_up.weight']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_vect = torch.randn(1,320).cuda()\n",
    "out1 = (rand_vect @ down1.T) @ up1.T\n",
    "out2 = (rand_vect.cpu() @ down2.T) @ up2.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = out1.cpu() - out2.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.1409e-08, -1.9558e-08,  5.0291e-08, -4.4703e-08, -3.8184e-08,\n",
       "         -1.7695e-08,  6.5193e-09, -1.3970e-08,  2.6077e-08,  3.8766e-08,\n",
       "          1.7695e-08, -1.3039e-08, -4.7730e-08, -2.7008e-08,  6.5891e-08,\n",
       "         -2.9337e-08,  1.1642e-08, -7.9162e-08, -2.7940e-08, -7.4506e-08,\n",
       "          2.3283e-08, -1.2107e-08,  7.8930e-08,  1.8626e-09, -2.2817e-08,\n",
       "         -4.0978e-08, -3.0268e-08,  1.6764e-08,  6.5193e-09,  2.5146e-08,\n",
       "         -3.4459e-08, -1.6764e-08,  1.5832e-08, -1.9092e-08,  2.3516e-08,\n",
       "          4.7497e-08,  7.9803e-08, -2.5146e-08,  3.8184e-08,  1.0012e-08,\n",
       "         -1.1525e-08, -3.7486e-08, -1.8626e-08,  5.8208e-09, -2.7008e-08,\n",
       "          1.7462e-08, -4.7497e-08, -1.7695e-08,  9.8604e-08,  7.2177e-08,\n",
       "         -9.4064e-08, -3.0734e-08,  4.6566e-09, -2.4913e-08,  3.0966e-08,\n",
       "         -5.3318e-08, -9.3132e-09, -4.3772e-08,  5.4017e-08,  1.4901e-08,\n",
       "         -1.2107e-08,  5.1223e-08, -4.6566e-10, -5.3085e-08, -2.1420e-08,\n",
       "          3.4925e-09, -1.0710e-08,  2.7940e-09, -4.4703e-08,  2.7707e-08,\n",
       "         -3.2131e-08, -3.5390e-08,  7.5437e-08,  1.2200e-07, -2.8871e-08,\n",
       "          4.0978e-08,  6.5193e-08,  7.5670e-09,  3.3528e-08,  4.4005e-08,\n",
       "          2.7940e-09, -2.2585e-08,  3.7253e-08, -4.1910e-08,  1.8626e-09,\n",
       "         -7.2643e-08,  4.6566e-08, -4.6566e-09, -7.1013e-09,  1.3039e-08,\n",
       "         -2.3283e-08, -8.3819e-09,  2.5961e-08,  4.5635e-08,  2.0955e-08,\n",
       "         -8.4750e-08,  3.3528e-08, -1.8044e-09, -1.0058e-07, -3.7253e-09,\n",
       "         -2.7940e-09,  3.3062e-08, -1.5832e-08,  5.1456e-08, -2.2352e-08,\n",
       "          2.0489e-08,  3.4925e-09, -9.3132e-10, -1.9558e-08,  2.2585e-08,\n",
       "          2.5146e-08, -4.8196e-08,  4.9826e-08, -2.2352e-08, -4.1910e-09,\n",
       "          3.7253e-08, -1.3970e-08,  5.8673e-08, -4.0047e-08,  5.0291e-08,\n",
       "         -2.4680e-08,  5.4482e-08, -4.0978e-08, -3.9116e-08, -2.0489e-08,\n",
       "          5.4017e-08,  2.7008e-08, -2.4214e-08,  8.0094e-08, -9.3132e-09,\n",
       "          7.9162e-09,  2.7008e-08, -4.5984e-09,  1.7695e-08, -2.7008e-08,\n",
       "         -2.3283e-10,  5.3085e-08, -1.7695e-08,  1.2340e-08, -1.7695e-08,\n",
       "          4.5169e-08, -2.8405e-08,  5.8673e-08,  3.6322e-08,  4.6566e-09,\n",
       "          6.6822e-08,  2.9802e-08,  2.7707e-08, -4.0978e-08, -1.0151e-07,\n",
       "          1.0245e-08, -3.3760e-08,  2.4214e-08, -3.3411e-08,  3.5390e-08,\n",
       "          1.2713e-07,  4.6450e-08, -7.8231e-08,  3.1665e-08, -2.9337e-08,\n",
       "         -5.7276e-08,  3.4692e-08, -8.0094e-08, -9.7789e-09, -1.5832e-08,\n",
       "         -9.5461e-09, -1.6764e-08,  4.6566e-08, -8.6147e-09,  1.1059e-08,\n",
       "          7.0781e-08,  3.9465e-08, -2.4214e-08, -2.0489e-08, -1.9558e-08,\n",
       "          1.8626e-09, -3.7253e-08, -1.6764e-08,  1.0245e-08, -3.7835e-09,\n",
       "         -5.2154e-08, -2.6077e-08,  2.9337e-08,  3.1665e-08,  4.6566e-08,\n",
       "         -1.0245e-08, -2.2817e-08, -5.1223e-09,  3.6322e-08, -4.1910e-08,\n",
       "         -1.9558e-08,  2.2817e-08, -1.2107e-07,  5.3085e-08,  1.1874e-08,\n",
       "         -1.1525e-08, -5.2503e-08,  2.3283e-10,  9.3132e-10, -4.1910e-08,\n",
       "         -4.6566e-09,  3.8184e-08, -1.5658e-07,  8.1491e-08, -2.3749e-08,\n",
       "          1.3597e-07, -8.8476e-09,  7.4506e-09, -1.1642e-08,  1.6764e-07,\n",
       "         -6.3330e-08,  1.1362e-07, -7.7416e-08,  2.7940e-09,  3.3993e-08,\n",
       "         -5.4948e-08, -6.1002e-08, -4.8894e-08,  1.1828e-07,  9.3132e-10,\n",
       "          7.9162e-09,  6.4261e-08, -6.5193e-09, -6.6007e-08, -2.9802e-08,\n",
       "          1.7928e-08,  7.7300e-08,  4.0047e-08, -1.3039e-08,  1.7695e-08,\n",
       "         -2.8871e-08, -5.5879e-08, -1.1036e-07,  4.9360e-08,  5.0059e-09,\n",
       "         -5.3551e-09, -5.5879e-08,  1.2456e-08,  3.3178e-08, -4.0978e-08,\n",
       "         -7.7998e-08,  3.7253e-08, -4.0047e-08,  2.6776e-08,  2.7940e-09,\n",
       "          1.3039e-08,  4.6100e-08, -3.1199e-08,  7.7765e-08, -2.1886e-08,\n",
       "         -9.3132e-09,  3.3528e-08,  1.1409e-08,  6.8918e-08, -4.0978e-08,\n",
       "         -1.7229e-08, -9.4529e-08, -3.2596e-08,  6.7055e-08, -2.9569e-08,\n",
       "          1.0151e-07, -9.7323e-08,  1.6647e-08,  4.2259e-08,  4.4703e-08,\n",
       "         -2.8871e-08,  2.6077e-08, -5.5879e-08,  1.9383e-08,  2.6077e-08,\n",
       "          9.2201e-08,  2.6193e-08,  9.6858e-08, -2.3283e-09,  1.1409e-08,\n",
       "         -4.5286e-08, -4.8429e-08, -4.4703e-08, -3.0617e-08, -2.9337e-08,\n",
       "          7.6368e-08, -3.9116e-08,  3.8184e-08, -5.4948e-08, -4.4703e-08,\n",
       "         -6.7055e-08, -2.3283e-08,  3.4808e-08, -2.6776e-08,  5.8906e-08,\n",
       "          4.1677e-08, -4.9360e-08, -8.1491e-08, -3.1432e-09,  5.5414e-08,\n",
       "         -3.0268e-08,  1.4063e-07,  2.5146e-08, -2.0256e-08, -1.2107e-08,\n",
       "          1.0105e-07,  1.3970e-08,  7.4506e-08,  3.8184e-08,  5.6811e-08,\n",
       "         -2.2817e-08, -4.5635e-08, -1.4901e-08,  1.0477e-09, -1.4668e-08,\n",
       "          3.4925e-09,  6.9849e-10,  4.8429e-08, -1.7695e-08, -2.8405e-08,\n",
       "          1.8626e-09, -2.3283e-08,  2.9337e-08, -1.6298e-08, -2.6776e-08]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "path = \"/home/ubuntu/AutoLoRADiscovery/lora_bundle.pt\"\n",
    "\n",
    "state_dict = torch.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 136/136 [00:00<00:00, 1758.83it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(len(state_dict))):\n",
    "    one = state_dict[i]\n",
    "\n",
    "    for k in list(one.keys()):\n",
    "        if \"lora_down\" in k or \"lora_up\" in k:\n",
    "            one[k] = one[k].T\n",
    "\n",
    "    state_dict[i] = one\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(state_dict, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
